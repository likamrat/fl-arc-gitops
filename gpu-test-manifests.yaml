---
# GPU Test Deployment - Example of using GPU in Kubernetes
# This demonstrates how to request GPU access in pod specifications

apiVersion: v1
kind: Namespace
metadata:
  name: gpu-test
---
# Simple GPU test pod that runs nvidia-smi
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test-cuda
  namespace: gpu-test
  labels:
    app: gpu-test
spec:
  restartPolicy: OnFailure
  # Use nvidia runtime to access GPU
  runtimeClassName: nvidia
  containers:
  - name: cuda-test
    image: nvidia/cuda:12.8.1-runtime-ubuntu24.04
    command: ["nvidia-smi"]
    resources:
      limits:
        # Request 1 GPU
        nvidia.com/gpu: 1
---
# Example: GPU workload that runs a CUDA computation
apiVersion: v1
kind: Pod
metadata:
  name: gpu-compute-test
  namespace: gpu-test
  labels:
    app: gpu-compute
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-compute
    # NVIDIA CUDA samples image
    image: nvidia/cuda:12.8.1-devel-ubuntu24.04
    command: ["/bin/bash", "-c"]
    args:
      - |
        echo "Running CUDA Vector Add Example..."
        cd /usr/local/cuda/samples/1_Utilities/deviceQuery
        make
        ./deviceQuery
    resources:
      limits:
        nvidia.com/gpu: 1
---
# Example: Multi-GPU workload (if you have multiple GPUs)
apiVersion: v1
kind: Pod
metadata:
  name: gpu-multi-test
  namespace: gpu-test
  labels:
    app: gpu-multi
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-multi
    image: nvidia/cuda:12.8.1-runtime-ubuntu24.04
    command: ["nvidia-smi"]
    # Request all available GPUs (adjust number as needed)
    resources:
      limits:
        nvidia.com/gpu: 1
